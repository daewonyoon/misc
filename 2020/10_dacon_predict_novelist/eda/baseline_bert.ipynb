{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.3 64-bit ('venv3764')",
   "display_name": "Python 3.7.3 64-bit ('venv3764')",
   "metadata": {
    "interpreter": {
     "hash": "01547ed404893b53106f3b9bafb45b1db5693eab43ca456b2077a32b371ee484"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "\n",
    "tf.random.set_seed(9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chk_dir_path()->str:\n",
    "    return os.path.abspath('../chk')    \n",
    "\n",
    "def get_dat_dir_path()->str:\n",
    "    return os.path.abspath('../dat')\n",
    "\n",
    "def get_train_csv_path()->str:\n",
    "    dat_dir = get_dat_dir_path()\n",
    "    # print(dat_dir)\n",
    "    return os.path.join(dat_dir, \"train.csv\")\n",
    "\n",
    "def get_test_csv_path()->str:\n",
    "    dat_dir = get_dat_dir_path()\n",
    "    return os.path.join(dat_dir, \"test_x.csv\")\n",
    "\n",
    "def get_sample_sub_path()->str:\n",
    "    dat_dir = get_dat_dir_path()\n",
    "    return os.path.join(dat_dir, \"sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 213k/213k [00:00<00:00, 354kB/s] \n"
     ]
    }
   ],
   "source": [
    "#### 텐서플로2 자연어처리 7장\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "\n",
    "\n",
    "def bert_tokenizer(sent: str, max_length: int):\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text=sent,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    input_id = encoded_dict[\"input_ids\"]\n",
    "    attention_mask = encoded_dict[\"attention_mask\"]\n",
    "    token_type_id = encoded_dict[\"token_type_ids\"]\n",
    "\n",
    "    return input_id, attention_mask, token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[101, 8667, 4542, 1139, 1385, 1910, 119, 102]\n[CLS] Hello darkness my old friend. [SEP]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"Hello darkness my old friend.\")\n",
    "print(encoded)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[CLS] What should I do for a mended heart, dear [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "r = bert_tokenizer(\"What should I do for a mended heart, dear\", 40)\n",
    "tokenizer.decode(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(get_train_csv_path())\n",
    "test = pd.read_csv(get_test_csv_path())\n",
    "sample_submission = pd.read_csv(get_sample_sub_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocesscing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_num(txt:str)->str:\n",
    "    return re.sub(r\"[^A-Za-z0-9 ]\", \"\", txt)\n",
    "\n",
    "train[\"text\"] = train[\"text\"].apply(alpha_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       index                                               text  author\n",
       "0          0  He was almost choking There was so much so muc...       3\n",
       "1          1                 Your sister asked for it I suppose       2\n",
       "2          2   She was engaged one day as she walked in peru...       1\n",
       "3          3  The captain was in the porch keeping himself c...       4\n",
       "4          4  Have mercy gentlemen odin flung up his hands D...       3\n",
       "...      ...                                                ...     ...\n",
       "54874  54874  Is that you Mr Smith odin whispered I hardly d...       2\n",
       "54875  54875  I told my plan to the captain and between us w...       4\n",
       "54876  54876   Your sincere wellwisher friend and sister LUC...       1\n",
       "54877  54877               Then you wanted me to lend you money       3\n",
       "54878  54878  It certainly had not occurred to me before but...       0\n",
       "\n",
       "[54879 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>text</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>He was almost choking There was so much so muc...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Your sister asked for it I suppose</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>She was engaged one day as she walked in peru...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>The captain was in the porch keeping himself c...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Have mercy gentlemen odin flung up his hands D...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>54874</th>\n      <td>54874</td>\n      <td>Is that you Mr Smith odin whispered I hardly d...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>54875</th>\n      <td>54875</td>\n      <td>I told my plan to the captain and between us w...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>54876</th>\n      <td>54876</td>\n      <td>Your sincere wellwisher friend and sister LUC...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>54877</th>\n      <td>54877</td>\n      <td>Then you wanted me to lend you money</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>54878</th>\n      <td>54878</td>\n      <td>It certainly had not occurred to me before but...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>54879 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \n",
    "             \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \n",
    "             \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \n",
    "             \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \n",
    "             \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \n",
    "             \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \n",
    "             \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \n",
    "             \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \n",
    "             \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \n",
    "             \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \n",
    "             \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "\n",
    "def remove_stopwords(text:str) -> str:\n",
    "    final_txt = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stopwords:\n",
    "            final_txt.append(i.strip())\n",
    "    return \" \".join(final_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text\"] = train[\"text\"].apply(alpha_num).apply(remove_stopwords)\n",
    "test[\"text\"] = test[\"text\"].apply(alpha_num).apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       index                                               text  author\n",
       "0          0  almost choking much much wanted say strange ex...       3\n",
       "1          1                               sister asked suppose       2\n",
       "2          2  engaged one day walked perusing Janes last let...       1\n",
       "3          3  captain porch keeping carefully way treacherou...       4\n",
       "4          4  mercy gentlemen odin flung hands Dont write an...       3\n",
       "...      ...                                                ...     ...\n",
       "54874  54874     Mr Smith odin whispered hardly dared hope come       2\n",
       "54875  54875  told plan captain us settled details accomplis...       4\n",
       "54876  54876         sincere wellwisher friend sister LUCY odin       1\n",
       "54877  54877                                  wanted lend money       3\n",
       "54878  54878               certainly not occurred said Yes like       0\n",
       "\n",
       "[54879 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>text</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>almost choking much much wanted say strange ex...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>sister asked suppose</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>engaged one day walked perusing Janes last let...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>captain porch keeping carefully way treacherou...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>mercy gentlemen odin flung hands Dont write an...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>54874</th>\n      <td>54874</td>\n      <td>Mr Smith odin whispered hardly dared hope come</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>54875</th>\n      <td>54875</td>\n      <td>told plan captain us settled details accomplis...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>54876</th>\n      <td>54876</td>\n      <td>sincere wellwisher friend sister LUCY odin</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>54877</th>\n      <td>54877</td>\n      <td>wanted lend money</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>54878</th>\n      <td>54878</td>\n      <td>certainly not occurred said Yes like</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>54879 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = train[\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([x for x in train[\"text\"]])\n",
    "x_test = np.array([x for x in test[\"text\"]])\n",
    "y_train = np.array([x for x in train[\"author\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "embedding_dim = 128\n",
    "max_length = 500\n",
    "padding_type = \"post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = Tokenizer(num_words=vocab_size)\n",
    "#tokenizer.fit_on_texts(x_train)\n",
    "#word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['odin', 'not', 'said', 'no', 'one', 'mr', 'will', 'upon', 'now', 'man']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "#list(word_index)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = tokenizer.texts_to_sequences(x_train)\n",
    "train_padded = pad_sequences(train_sequence, padding=padding_type, maxlen=max_length)\n",
    "\n",
    "test_sequence = tokenizer.texts_to_sequences(x_test)\n",
    "test_padded = pad_sequences(test_sequence, padding=padding_type, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[  141,  7259,    20, ...,     0,     0,     0],\n",
       "       [  217,    58,   221, ...,     0,     0,     0],\n",
       "       [  682,     5,    59, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [ 2407, 19738,   126, ...,     0,     0,     0],\n",
       "       [  316,  3532,   164, ...,     0,     0,     0],\n",
       "       [  203,     2,   811, ...,     0,     0,     0]])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## this model part is from 텐서플로2와 머신러닝으로 시작하는 자연어처리 07. 텍스트분류\n",
    "############################################################################\n",
    "\n",
    "model_name = \"cnn_classifier_en\"\n",
    "BATCH_SIZE=512\n",
    "NUM_EPOCHS=10\n",
    "VALID_SPLIT=0.1\n",
    "MAX_LEN=train_padded.shape[1]\n",
    "\n",
    "kargs = {\n",
    "    \"model_name\": model_name,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embedding_size\": 256,\n",
    "    \"num_filters\": 100,\n",
    "    \"dropout_rate\": .5,\n",
    "    \"hidden_dimension\": 250,\n",
    "    \"output_dimension\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TBertClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super(TBertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(num_class,\n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initialier_range),\n",
    "                                                name=\"classifier\")\n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output, training=training)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(tf.keras.Model):\n",
    "    def __init__(self, **kargs):\n",
    "        super(CNNClassifier, self).__init__(name=kargs[\"model_name\"])\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=kargs[\"vocab_size\"],\n",
    "                                                   output_dim=kargs[\"embedding_size\"])\n",
    "        self.conv_list = [ tf.keras.layers.Conv1D(filters=kargs[\"num_filters\"],\n",
    "                                                  kernel_size=kernel_size,   \n",
    "                                                  padding=\"valid\", \n",
    "                                                  activation=\"relu\", \n",
    "                                                  kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "                            for kernel_size in [3,4,5,6,7] ]\n",
    "        self.pooling = tf.keras.layers.GlobalMaxPooling1D()\n",
    "        self.dropout = tf.keras.layers.Dropout(kargs[\"dropout_rate\"])\n",
    "        self.fc1 = tf.keras.layers.Dense(units=kargs[\"hidden_dimension\"], activation=\"relu\", \n",
    "                                         kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.)\n",
    "                                         )\n",
    "        self.fc2 = tf.keras.layers.Dense(units=kargs[\"output_dimension\"], activation=\"softmax\",\n",
    "                                         kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.)\n",
    "                                         )\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = tf.concat([self.pooling(conv(x)) for conv in self.conv_list], axis=-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x                                         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNClassifier(**kargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"cnn_classifier_en\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        multiple                  5120000   \n_________________________________________________________________\nconv1d (Conv1D)              multiple                  76900     \n_________________________________________________________________\nconv1d_1 (Conv1D)            multiple                  102500    \n_________________________________________________________________\nconv1d_2 (Conv1D)            multiple                  128100    \n_________________________________________________________________\nconv1d_3 (Conv1D)            multiple                  153700    \n_________________________________________________________________\nconv1d_4 (Conv1D)            multiple                  179300    \n_________________________________________________________________\nglobal_max_pooling1d (Global multiple                  0         \n_________________________________________________________________\ndropout (Dropout)            multiple                  0         \n_________________________________________________________________\ndense (Dense)                multiple                  125250    \n_________________________________________________________________\ndense_1 (Dense)              multiple                  1255      \n=================================================================\nTotal params: 5,887,005\nTrainable params: 5,887,005\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.build(train_padded.shape)\n",
    "print(model.summary())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.59457, saving model to d:\\github\\daewonyoon\\misc\\2020\\10_dacon_predict_novelist\\chk\\cnn_classifier_en\\weights.h5\n",
      "97/97 - 41s - loss: 1.3976 - accuracy: 0.4045 - val_loss: 1.0473 - val_accuracy: 0.5946\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.59457 to 0.73069, saving model to d:\\github\\daewonyoon\\misc\\2020\\10_dacon_predict_novelist\\chk\\cnn_classifier_en\\weights.h5\n",
      "97/97 - 40s - loss: 0.8422 - accuracy: 0.6870 - val_loss: 0.7382 - val_accuracy: 0.7307\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.73069 to 0.74289, saving model to d:\\github\\daewonyoon\\misc\\2020\\10_dacon_predict_novelist\\chk\\cnn_classifier_en\\weights.h5\n",
      "97/97 - 40s - loss: 0.5750 - accuracy: 0.7934 - val_loss: 0.6951 - val_accuracy: 0.7429\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.74289\n",
      "97/97 - 40s - loss: 0.4470 - accuracy: 0.8397 - val_loss: 0.7061 - val_accuracy: 0.7403\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.74289\n",
      "97/97 - 40s - loss: 0.3691 - accuracy: 0.8681 - val_loss: 0.7381 - val_accuracy: 0.7392\n"
     ]
    }
   ],
   "source": [
    "#num_epochs = 20\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", \n",
    "                                                    min_delta=0.0001, \n",
    "                                                    patience=2)\n",
    "\n",
    "checkpoint_path = os.path.join( get_chk_dir_path(), model_name, \"weights.h5\" )\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                monitor=\"val_accuracy\", \n",
    "                                                verbose=1, \n",
    "                                                save_best_only=True, \n",
    "                                                save_weights_only=True)\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "history = model.fit(train_padded, y_train, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=NUM_EPOCHS, \n",
    "                    verbose=2, \n",
    "                    validation_split=VALID_SPLIT, \n",
    "                    callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "pred = model.predict(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.7089713e-02, 2.5950196e-01, 5.0526047e-01, 2.0786697e-01,\n",
       "        1.0280924e-02],\n",
       "       [1.1359072e-01, 7.1428502e-01, 2.9176155e-02, 5.2005421e-02,\n",
       "        9.0942726e-02],\n",
       "       [9.1241038e-01, 8.2863055e-02, 1.2584317e-03, 5.5661576e-04,\n",
       "        2.9115642e-03],\n",
       "       ...,\n",
       "       [1.0660500e-03, 9.9865985e-01, 9.4617180e-06, 2.6123933e-04,\n",
       "        3.3049264e-06],\n",
       "       [3.9650548e-02, 9.2917782e-01, 3.3951139e-03, 2.6424885e-02,\n",
       "        1.3515840e-03],\n",
       "       [9.5895571e-01, 9.5104389e-03, 5.2798851e-03, 7.5844228e-03,\n",
       "        1.8669603e-02]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(19617, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(19617, 500)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       index         0         1         2         3         4\n",
       "0          0  0.017090  0.259502  0.505260  0.207867  0.010281\n",
       "1          1  0.113591  0.714285  0.029176  0.052005  0.090943\n",
       "2          2  0.912410  0.082863  0.001258  0.000557  0.002912\n",
       "3          3  0.068767  0.001953  0.730606  0.006248  0.192426\n",
       "4          4  0.487861  0.066780  0.018471  0.332050  0.094838\n",
       "...      ...       ...       ...       ...       ...       ...\n",
       "19612  19612  0.004014  0.994086  0.000073  0.001731  0.000096\n",
       "19613  19613  0.113119  0.008009  0.029976  0.001777  0.847118\n",
       "19614  19614  0.001066  0.998660  0.000009  0.000261  0.000003\n",
       "19615  19615  0.039651  0.929178  0.003395  0.026425  0.001352\n",
       "19616  19616  0.958956  0.009510  0.005280  0.007584  0.018670\n",
       "\n",
       "[19617 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.017090</td>\n      <td>0.259502</td>\n      <td>0.505260</td>\n      <td>0.207867</td>\n      <td>0.010281</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.113591</td>\n      <td>0.714285</td>\n      <td>0.029176</td>\n      <td>0.052005</td>\n      <td>0.090943</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.912410</td>\n      <td>0.082863</td>\n      <td>0.001258</td>\n      <td>0.000557</td>\n      <td>0.002912</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.068767</td>\n      <td>0.001953</td>\n      <td>0.730606</td>\n      <td>0.006248</td>\n      <td>0.192426</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.487861</td>\n      <td>0.066780</td>\n      <td>0.018471</td>\n      <td>0.332050</td>\n      <td>0.094838</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19612</th>\n      <td>19612</td>\n      <td>0.004014</td>\n      <td>0.994086</td>\n      <td>0.000073</td>\n      <td>0.001731</td>\n      <td>0.000096</td>\n    </tr>\n    <tr>\n      <th>19613</th>\n      <td>19613</td>\n      <td>0.113119</td>\n      <td>0.008009</td>\n      <td>0.029976</td>\n      <td>0.001777</td>\n      <td>0.847118</td>\n    </tr>\n    <tr>\n      <th>19614</th>\n      <td>19614</td>\n      <td>0.001066</td>\n      <td>0.998660</td>\n      <td>0.000009</td>\n      <td>0.000261</td>\n      <td>0.000003</td>\n    </tr>\n    <tr>\n      <th>19615</th>\n      <td>19615</td>\n      <td>0.039651</td>\n      <td>0.929178</td>\n      <td>0.003395</td>\n      <td>0.026425</td>\n      <td>0.001352</td>\n    </tr>\n    <tr>\n      <th>19616</th>\n      <td>19616</td>\n      <td>0.958956</td>\n      <td>0.009510</td>\n      <td>0.005280</td>\n      <td>0.007584</td>\n      <td>0.018670</td>\n    </tr>\n  </tbody>\n</table>\n<p>19617 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "sample_submission[[str(i) for i in range(5)]] = pred\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission_cnn.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}