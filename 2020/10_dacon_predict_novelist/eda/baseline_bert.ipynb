{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.3 64-bit ('venv3764')",
   "display_name": "Python 3.7.3 64-bit ('venv3764')",
   "metadata": {
    "interpreter": {
     "hash": "01547ed404893b53106f3b9bafb45b1db5693eab43ca456b2077a32b371ee484"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tf.random.set_seed(9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chk_dir_path()->str:\n",
    "    return os.path.abspath('../chk')    \n",
    "\n",
    "def get_dat_dir_path()->str:\n",
    "    return os.path.abspath('../dat')\n",
    "\n",
    "def get_train_csv_path()->str:\n",
    "    dat_dir = get_dat_dir_path()\n",
    "    # print(dat_dir)\n",
    "    return os.path.join(dat_dir, \"train.csv\")\n",
    "\n",
    "def get_test_csv_path()->str:\n",
    "    dat_dir = get_dat_dir_path()\n",
    "    return os.path.join(dat_dir, \"test_x.csv\")\n",
    "\n",
    "def get_sample_sub_path()->str:\n",
    "    dat_dir = get_dat_dir_path()\n",
    "    return os.path.join(dat_dir, \"sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "VALID_SPLIT = 0.2\n",
    "MAX_LEN = 500\n",
    "BERT_PRETRAIND_MODEL = \"bert-large-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 텐서플로2 자연어처리 7장\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PRETRAIND_MODEL)\n",
    "\n",
    "\n",
    "def bert_tokenizer(sent: str, max_length: int):\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text=sent,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    input_id = encoded_dict[\"input_ids\"]\n",
    "    attention_mask = encoded_dict[\"attention_mask\"]\n",
    "    token_type_id = encoded_dict[\"token_type_ids\"]\n",
    "\n",
    "    return input_id, attention_mask, token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[101, 8667, 4542, 1139, 1385, 1910, 119, 102]\n[CLS] Hello darkness my old friend. [SEP]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"Hello darkness my old friend.\")\n",
    "print(encoded)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[CLS] What should I do for a mended heart, dear [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "r = bert_tokenizer(\"What should I do for a mended heart, dear\", 40)\n",
    "tokenizer.decode(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(get_train_csv_path())\n",
    "test = pd.read_csv(get_test_csv_path())\n",
    "sample_submission = pd.read_csv(get_sample_sub_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index                                               text  author\n",
       "0      0  He was almost choking. There was so much, so m...       3\n",
       "1      1             “Your sister asked for it, I suppose?”       2\n",
       "2      2   She was engaged one day as she walked, in per...       1\n",
       "3      3  The captain was in the porch, keeping himself ...       4\n",
       "4      4  “Have mercy, gentlemen!” odin flung up his han...       3"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>text</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>He was almost choking. There was so much, so m...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>“Your sister asked for it, I suppose?”</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>She was engaged one day as she walked, in per...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>The captain was in the porch, keeping himself ...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index                                               text\n",
       "0      0  “Not at all. I think she is one of the most ch...\n",
       "1      1  \"No,\" replied he, with sudden consciousness, \"...\n",
       "2      2  As the lady had stated her intention of scream...\n",
       "3      3  “And then suddenly in the silence I heard a so...\n",
       "4      4  His conviction remained unchanged. So far as I..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>“Not at all. I think she is one of the most ch...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>As the lady had stated her intention of scream...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>“And then suddenly in the silence I heard a so...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>His conviction remained unchanged. So far as I...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_texts_to_bert_inputs(texts:pd.Series, MAX_LEN:int):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "\n",
    "    for train_sent in tqdm(texts):\n",
    "        try:\n",
    "            input_id, attention_mask, token_type_id = bert_tokenizer(train_sent, MAX_LEN)\n",
    "            \n",
    "            input_ids.append(input_id)\n",
    "            attention_masks.append(attention_mask)\n",
    "            token_type_ids.append(token_type_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(train_sent)\n",
    "            pass\n",
    "\n",
    "    train_movie_input_ids = np.array(input_ids, dtype=int)\n",
    "    train_movie_attention_masks = np.array(attention_masks, dtype=int)\n",
    "    train_movie_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    train_movie_inputs = (train_movie_input_ids, train_movie_attention_masks, train_movie_type_ids)\n",
    "\n",
    "    return train_movie_inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 54879/54879 [00:42<00:00, 1302.47it/s]\n",
      "100%|██████████| 19617/19617 [00:29<00:00, 665.23it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_bert_inputs = transform_texts_to_bert_inputs(train[\"text\"], 500)\n",
    "test_bert_inputs = transform_texts_to_bert_inputs(test[\"text\"], 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocesscingᆼ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def alpha_num(txt:str)->str:\n",
    "#    return re.sub(r\"[^A-Za-z0-9 ]\", \"\", txt)\n",
    "\n",
    "#train[\"text\"] = train[\"text\"].apply(alpha_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train[\"text\"] = train[\"text\"].apply(alpha_num)\n",
    "#test[\"text\"] = test[\"text\"].apply(alpha_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = train[\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = np.array([x for x in train[\"text\"]])\n",
    "#x_test = np.array([x for x in test[\"text\"]])\n",
    "#y_train = np.array([x for x in train[\"author\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size = 20000\n",
    "#embedding_dim = 128\n",
    "##max_length = 500\n",
    "#padding_type = \"post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = Tokenizer(num_words=vocab_size)\n",
    "#tokenizer.fit_on_texts(x_train)\n",
    "#word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(word_index)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sequence = tokenizer.texts_to_sequences(x_train)\n",
    "#train_padded = pad_sequences(train_sequence, padding=padding_type, maxlen=max_length)\n",
    "\n",
    "#test_sequence = tokenizer.texts_to_sequences(x_test)\n",
    "#test_padded = pad_sequences(test_sequence, padding=padding_type, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## this model part is from 텐서플로2와 머신러닝으로 시작하는 자연어처리 07. 텍스트분류\n",
    "############################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super(TFBertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(num_class,\n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range),\n",
    "                                                name=\"classifier\")\n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output, training=training)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at bert-large-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertClassifier(model_name=BERT_PRETRAIND_MODEL, dir_path=\"bert_ckpt\", num_class=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 준비하기\n",
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.build(train_padded.shape)\n",
    "#print(model.summary())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/daewonyoon/github/daewonyoon/misc/2020/10_dacon_predict_novelist/chk/tf2_bert_naver_movie -- Folder already exists \n\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'cls_model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-3201bb8c6a20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 학습과 eval 시작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m history = cls_model.fit(train_bert_inputs, train[\"author\"], epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n\u001b[0m\u001b[1;32m     23\u001b[0m                     validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cls_model' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = \"bert_classifier_en\"\n",
    "\n",
    "# overfitting을 막기 위한 ealrystop 추가\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=2)\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
    "\n",
    "checkpoint_path = os.path.join( get_chk_dir_path(), model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# 학습과 eval 시작\n",
    "history = model.fit(train_bert_inputs, train[\"author\"], epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                    validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\n",
    "\n",
    "#steps_for_epoch\n",
    "\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.59457, saving model to d:\\github\\daewonyoon\\misc\\2020\\10_dacon_predict_novelist\\chk\\cnn_classifier_en\\weights.h5\n",
      "97/97 - 41s - loss: 1.3976 - accuracy: 0.4045 - val_loss: 1.0473 - val_accuracy: 0.5946\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.59457 to 0.73069, saving model to d:\\github\\daewonyoon\\misc\\2020\\10_dacon_predict_novelist\\chk\\cnn_classifier_en\\weights.h5\n",
      "97/97 - 40s - loss: 0.8422 - accuracy: 0.6870 - val_loss: 0.7382 - val_accuracy: 0.7307\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.73069 to 0.74289, saving model to d:\\github\\daewonyoon\\misc\\2020\\10_dacon_predict_novelist\\chk\\cnn_classifier_en\\weights.h5\n",
      "97/97 - 40s - loss: 0.5750 - accuracy: 0.7934 - val_loss: 0.6951 - val_accuracy: 0.7429\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.74289\n",
      "97/97 - 40s - loss: 0.4470 - accuracy: 0.8397 - val_loss: 0.7061 - val_accuracy: 0.7403\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.74289\n",
      "97/97 - 40s - loss: 0.3691 - accuracy: 0.8681 - val_loss: 0.7381 - val_accuracy: 0.7392\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#num_epochs = 20\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", \n",
    "                                                    min_delta=0.0001, \n",
    "                                                    patience=2)\n",
    "\n",
    "checkpoint_path = os.path.join( get_chk_dir_path(), model_name, \"weights.h5\" )\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                monitor=\"val_accuracy\", \n",
    "                                                verbose=1, \n",
    "                                                save_best_only=True, \n",
    "                                                save_weights_only=True)\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "history = model.fit(train_padded, y_train, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=NUM_EPOCHS, \n",
    "                    verbose=2, \n",
    "                    validation_split=VALID_SPLIT, \n",
    "                    callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "pred = model.predict(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.7089713e-02, 2.5950196e-01, 5.0526047e-01, 2.0786697e-01,\n",
       "        1.0280924e-02],\n",
       "       [1.1359072e-01, 7.1428502e-01, 2.9176155e-02, 5.2005421e-02,\n",
       "        9.0942726e-02],\n",
       "       [9.1241038e-01, 8.2863055e-02, 1.2584317e-03, 5.5661576e-04,\n",
       "        2.9115642e-03],\n",
       "       ...,\n",
       "       [1.0660500e-03, 9.9865985e-01, 9.4617180e-06, 2.6123933e-04,\n",
       "        3.3049264e-06],\n",
       "       [3.9650548e-02, 9.2917782e-01, 3.3951139e-03, 2.6424885e-02,\n",
       "        1.3515840e-03],\n",
       "       [9.5895571e-01, 9.5104389e-03, 5.2798851e-03, 7.5844228e-03,\n",
       "        1.8669603e-02]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(19617, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(19617, 500)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       index         0         1         2         3         4\n",
       "0          0  0.017090  0.259502  0.505260  0.207867  0.010281\n",
       "1          1  0.113591  0.714285  0.029176  0.052005  0.090943\n",
       "2          2  0.912410  0.082863  0.001258  0.000557  0.002912\n",
       "3          3  0.068767  0.001953  0.730606  0.006248  0.192426\n",
       "4          4  0.487861  0.066780  0.018471  0.332050  0.094838\n",
       "...      ...       ...       ...       ...       ...       ...\n",
       "19612  19612  0.004014  0.994086  0.000073  0.001731  0.000096\n",
       "19613  19613  0.113119  0.008009  0.029976  0.001777  0.847118\n",
       "19614  19614  0.001066  0.998660  0.000009  0.000261  0.000003\n",
       "19615  19615  0.039651  0.929178  0.003395  0.026425  0.001352\n",
       "19616  19616  0.958956  0.009510  0.005280  0.007584  0.018670\n",
       "\n",
       "[19617 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.017090</td>\n      <td>0.259502</td>\n      <td>0.505260</td>\n      <td>0.207867</td>\n      <td>0.010281</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.113591</td>\n      <td>0.714285</td>\n      <td>0.029176</td>\n      <td>0.052005</td>\n      <td>0.090943</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.912410</td>\n      <td>0.082863</td>\n      <td>0.001258</td>\n      <td>0.000557</td>\n      <td>0.002912</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.068767</td>\n      <td>0.001953</td>\n      <td>0.730606</td>\n      <td>0.006248</td>\n      <td>0.192426</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.487861</td>\n      <td>0.066780</td>\n      <td>0.018471</td>\n      <td>0.332050</td>\n      <td>0.094838</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19612</th>\n      <td>19612</td>\n      <td>0.004014</td>\n      <td>0.994086</td>\n      <td>0.000073</td>\n      <td>0.001731</td>\n      <td>0.000096</td>\n    </tr>\n    <tr>\n      <th>19613</th>\n      <td>19613</td>\n      <td>0.113119</td>\n      <td>0.008009</td>\n      <td>0.029976</td>\n      <td>0.001777</td>\n      <td>0.847118</td>\n    </tr>\n    <tr>\n      <th>19614</th>\n      <td>19614</td>\n      <td>0.001066</td>\n      <td>0.998660</td>\n      <td>0.000009</td>\n      <td>0.000261</td>\n      <td>0.000003</td>\n    </tr>\n    <tr>\n      <th>19615</th>\n      <td>19615</td>\n      <td>0.039651</td>\n      <td>0.929178</td>\n      <td>0.003395</td>\n      <td>0.026425</td>\n      <td>0.001352</td>\n    </tr>\n    <tr>\n      <th>19616</th>\n      <td>19616</td>\n      <td>0.958956</td>\n      <td>0.009510</td>\n      <td>0.005280</td>\n      <td>0.007584</td>\n      <td>0.018670</td>\n    </tr>\n  </tbody>\n</table>\n<p>19617 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "sample_submission[[str(i) for i in range(5)]] = pred\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission_cnn.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}